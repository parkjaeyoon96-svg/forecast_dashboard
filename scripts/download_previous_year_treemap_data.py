"""
íŠ¸ë¦¬ë§µ ì „ë…„ë¹„ ë°ì´í„°ë¥¼ Snowflakeì—ì„œ ì¡°íšŒ ë° ì „ì²˜ë¦¬í•˜ì—¬ CSVë¡œ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/download_previous_year_treemap_data.py 20251215
    
ì„¤ëª…:
    ì—…ë°ì´íŠ¸ì¼ì(YYYYMMDD)ë¥¼ ì…ë ¥í•˜ë©´:
    - ë‹¹ë…„ ê¸°ê°„: 2025-12-01 ~ 2025-12-14
    - ì „ë…„ ê¸°ê°„: 2024-12-01 ~ 2024-12-15 (ì—…ë°ì´íŠ¸ì¼ì í¬í•¨)
    
ë¡œì§:
    1. Snowflakeì—ì„œ ì „ë…„ ë™ì£¼ì°¨ì¼ê¹Œì§€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (DW_COPA_D)
    2. ë§ˆìŠ¤í„° ê¸°ë°˜ ì „ì²˜ë¦¬:
       - ì±„ë„ëª… ë§¤í•‘ (RF ì²˜ë¦¬ í¬í•¨)
       - ì•„ì´í…œ ì¤‘ë¶„ë¥˜/ì†Œë¶„ë¥˜ ë§¤í•‘
    3. ë¸Œëœë“œë³„, ì±„ë„ë³„, ì•„ì´í…œë³„ ì§‘ê³„
    4. CSV íŒŒì¼ë¡œ ì €ì¥
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
import snowflake.connector
import pandas as pd

# ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

env_path = project_root / '.env'
if env_path.exists():
    load_dotenv(env_path)
else:
    print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì½ìŠµë‹ˆë‹¤.")

def get_snowflake_connection():
    """Snowflake ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
    try:
        conn = snowflake.connector.connect(
            account=os.getenv('SNOWFLAKE_ACCOUNT'),
            user=os.getenv('SNOWFLAKE_USERNAME'),
            password=os.getenv('SNOWFLAKE_PASSWORD'),
            warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
            database=os.getenv('SNOWFLAKE_DATABASE')
        )
        print("âœ… Snowflake ì—°ê²° ì„±ê³µ!")
        return conn
    except Exception as e:
        print(f"âŒ Snowflake ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

def calculate_previous_year_period(update_date_str: str):
    """
    ì—…ë°ì´íŠ¸ì¼ìë¡œë¶€í„° ì „ë…„ ë™ê¸°ê°„ ê³„ì‚°
    
    ë‹¹ë…„: ë¶„ì„ì›”ì˜ 1ì¼ ~ ì—…ë°ì´íŠ¸ì¼ì˜ D-1ì¼ (ë‹¨, ë¶„ì„ì›” ë§ì¼ ì´ˆê³¼ ì‹œ ë§ì¼ë¡œ ì œí•œ)
    ì „ë…„: ë‹¹ë…„ê³¼ ë™ì¼ ê¸°ê°„ (ì „ë…„ë„ ë™ì¼ ì›”)
    
    ì˜ˆ: 
    - ì—…ë°ì´íŠ¸ì¼: 20260105, ë¶„ì„ì›”: 202512
    - ë‹¹ë…„: 2025-12-01 ~ 2025-12-31 (ì—…ë°ì´íŠ¸ì¼ ì „ë‚ ì´ ë§ì¼ ì´ˆê³¼í•˜ë¯€ë¡œ ë§ì¼ë¡œ ì œí•œ)
    - ì „ë…„: 2024-12-01 ~ 2024-12-31 (ë‹¹ë…„ê³¼ ë™ì¼ ì¼ìˆ˜)
    
    Args:
        update_date_str: YYYYMMDD í˜•ì‹ (ì˜ˆ: 20251215)
    
    Returns:
        tuple: (ì „ë…„_ì‹œì‘ì¼, ì „ë…„_ì¢…ë£Œì¼) YYYY-MM-DD í˜•ì‹
    """
    from calendar import monthrange
    
    # ì—…ë°ì´íŠ¸ì¼ì íŒŒì‹±
    update_date = datetime.strptime(update_date_str, '%Y%m%d')
    
    # â˜… ë¶„ì„ì›” ê³„ì‚°: metadata.jsonì—ì„œ ê°€ì ¸ì˜¤ê¸° â˜…
    analysis_month_str = update_date_str[:6]  # YYYYMM (ê¸°ë³¸ê°’)
    
    # metadata.jsonì—ì„œ ì‹¤ì œ ë¶„ì„ì›” í™•ì¸
    try:
        from path_utils import get_current_year_file_path
        metadata_path = get_current_year_file_path(update_date_str, 'metadata.json')
        if os.path.exists(metadata_path):
            import json
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                if 'analysis_month' in metadata:
                    analysis_month_str = metadata['analysis_month']
                    print(f"ğŸ“‹ metadata.jsonì—ì„œ ë¶„ì„ì›” í™•ì¸: {analysis_month_str}")
    except:
        pass
    
    # ë¶„ì„ì›”ì˜ ë…„ì›”ë¡œ ë‹¹ë…„ ê¸°ê°„ ì„¤ì •
    analysis_year = int(analysis_month_str[:4])
    analysis_month = int(analysis_month_str[4:6])
    
    # ë‹¹ë…„ ì‹œì‘ì¼: ë¶„ì„ì›”ì˜ 1ì¼
    current_start = datetime(analysis_year, analysis_month, 1)
    
    # ë‹¹ë…„ ì¢…ë£Œì¼: ì—…ë°ì´íŠ¸ì¼ì˜ D-1ì¼
    current_end_calc = update_date - timedelta(days=1)
    
    # ë¶„ì„ì›”ì˜ ë§ì¼
    last_day = monthrange(analysis_year, analysis_month)[1]
    current_month_end = datetime(analysis_year, analysis_month, last_day)
    
    # ì—…ë°ì´íŠ¸ì¼ ì „ë‚ ì´ ë¶„ì„ì›” ë§ì¼ì„ ì´ˆê³¼í•˜ë©´ ë§ì¼ë¡œ ì œí•œ
    if current_end_calc > current_month_end:
        current_end = current_month_end
    else:
        current_end = current_end_calc
    
    # ë‹¹ë…„ì˜ ì¼ìˆ˜ ê³„ì‚°
    current_days = (current_end - current_start).days + 1
    
    # ì „ë…„ ê¸°ê°„: ì „ë…„ë„ ë™ì¼ ì›”ì—ì„œ ë™ì£¼ì°¨ ê³„ì‚°
    prev_year = analysis_year - 1
    prev_month_start = datetime(prev_year, analysis_month, 1)
    
    # ë‹¹ë…„ ì‹œì‘ì¼ê³¼ ì „ë…„ ì›”ì´ˆì˜ ìš”ì¼ ì°¨ì´ ê³„ì‚°
    current_start_weekday = current_start.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    prev_month_start_weekday = prev_month_start.weekday()
    
    # ì „ë…„ ì‹œì‘ì¼: ì „ë…„ë„ í•´ë‹¹ ì›”ì—ì„œ ë‹¹ë…„ ì‹œì‘ì¼ê³¼ ë™ì¼í•œ ìš”ì¼ ì°¾ê¸°
    weekday_diff = current_start_weekday - prev_month_start_weekday
    if weekday_diff < 0:
        weekday_diff += 7
    prev_start = prev_month_start + timedelta(days=weekday_diff)
    
    # ì „ë…„ ì¢…ë£Œì¼: ì „ë…„ ì‹œì‘ì¼ë¡œë¶€í„° ë‹¹ë…„ê³¼ ë™ì¼í•œ ì¼ìˆ˜
    prev_end = prev_start + timedelta(days=current_days - 1)
    
    prev_days = (prev_end - prev_start).days + 1
    
    prev_start_str = prev_start.strftime('%Y-%m-%d')
    prev_end_str = prev_end.strftime('%Y-%m-%d')
    
    # ìš”ì¼ ì •ë³´
    weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
    current_start_name = weekday_names[current_start.weekday()]
    current_end_name = weekday_names[current_end.weekday()]
    prev_start_name = weekday_names[prev_start.weekday()]
    prev_end_name = weekday_names[prev_end.weekday()]
    
    print(f"ğŸ“… ë‹¹ë…„ ê¸°ê°„ ({analysis_month_str}ì›”): {current_start.strftime('%Y-%m-%d')}({current_start_name}) ~ {current_end.strftime('%Y-%m-%d')}({current_end_name}) - {current_days}ì¼")
    print(f"ğŸ“… ì „ë…„ ê¸°ê°„ ({prev_year}-{analysis_month:02d}ì›”): {prev_start_str}({prev_start_name}) ~ {prev_end_str}({prev_end_name}) - {prev_days}ì¼")
    
    return prev_start_str, prev_end_str

def get_treemap_previous_year_query(start_date: str, end_date: str):
    """
    íŠ¸ë¦¬ë§µ ì „ë…„ ë°ì´í„° ì¡°íšŒ ì¿¼ë¦¬ ìƒì„±
    DW_COPA_D í…Œì´ë¸”ì—ì„œ ë¸Œëœë“œ, ì±„ë„, ê³ ê°, ì•„ì´í…œ ê³„ì¸µë³„ë¡œ ì„¸ë¶€ ë°ì´í„° ì¡°íšŒ
    """
    query = f"""
SELECT
    BRD_CD AS "ë¸Œëœë“œì½”ë“œ",
    CASE 
        WHEN BRD_CD = 'ST' THEN SUBSTR(PRDT_CD, 3, 3)
        ELSE SUBSTR(PRDT_CD, 2, 3)
    END AS "ì‹œì¦Œ",
    CHNL_CD AS "ì±„ë„ì½”ë“œ",
    CUST_CD AS "ê³ ê°ì½”ë“œ",
    PRDT_HRRC_CD1 AS "prdt_hrrc_cd1",
    PRDT_HRRC_CD2 AS "prdt_hrrc_cd2",
    PRDT_HRRC_CD3 AS "prdt_hrrc_cd3",
    CASE
        WHEN BRD_CD = 'ST' THEN SUBSTR(PRDT_CD, 8, 2)
        ELSE SUBSTR(PRDT_CD, 7, 2)
    END AS "ì•„ì´í…œì½”ë“œ",
    SUM(TAG_SALE_AMT) AS "TAGë§¤ì¶œ",
    SUM(ACT_SALE_AMT) AS "ì‹¤íŒë§¤ì¶œ"
FROM FNF.SAP_FNF.DW_COPA_D
WHERE PST_DT BETWEEN '{start_date}' AND '{end_date}'
  AND CORP_CD = '1000'
  AND BRD_CD <> 'A'
  AND CHNL_CD <> '9'
  AND PRDT_HRRC_CD1 <> 'E0100'
GROUP BY
    BRD_CD,
    CASE 
        WHEN BRD_CD = 'ST' THEN SUBSTR(PRDT_CD, 3, 3)
        ELSE SUBSTR(PRDT_CD, 2, 3)
    END,
    CHNL_CD,
    CUST_CD,
    PRDT_HRRC_CD1,
    PRDT_HRRC_CD2,
    PRDT_HRRC_CD3,
    CASE
        WHEN BRD_CD = 'ST' THEN SUBSTR(PRDT_CD, 8, 2)
        ELSE SUBSTR(PRDT_CD, 7, 2)
    END
ORDER BY BRD_CD, CHNL_CD
"""
    return query

def execute_query_to_dataframe(conn, query: str):
    """ì¿¼ë¦¬ ì‹¤í–‰ ë° DataFrame ë°˜í™˜"""
    try:
        print("ğŸ“Š ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
        cursor = conn.cursor()
        cursor.execute(query)
        
        columns = [desc[0] for desc in cursor.description]
        print("ğŸ“¥ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        data = cursor.fetchall()
        df = pd.DataFrame(data, columns=columns)
        
        cursor.close()
        print(f"âœ… {len(df):,}ê±´ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.")
        return df
    except Exception as e:
        print(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

def load_channel_master():
    """ì±„ë„ ë§ˆìŠ¤í„° ë¡œë“œ"""
    master_path = project_root / "Master" / "ì±„ë„ë§ˆìŠ¤í„°.csv"
    if not master_path.exists():
        raise FileNotFoundError(f"ì±„ë„ ë§ˆìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {master_path}")
    
    df = pd.read_csv(master_path, encoding='utf-8-sig')
    print(f"âœ… ì±„ë„ ë§ˆìŠ¤í„° ë¡œë“œ: {len(df)}ê±´")
    return df

def load_item_master():
    """ì•„ì´í…œ ë§ˆìŠ¤í„° ë¡œë“œ"""
    master_path = project_root / "Master" / "ì•„ì´í…œë§ˆìŠ¤í„°.csv"
    if not master_path.exists():
        raise FileNotFoundError(f"ì•„ì´í…œ ë§ˆìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {master_path}")
    
    df = pd.read_csv(master_path, encoding='utf-8-sig')
    print(f"âœ… ì•„ì´í…œ ë§ˆìŠ¤í„° ë¡œë“œ: {len(df)}ê±´")
    return df

def map_channel_name(row, channel_master):
    """
    ì±„ë„ëª… ë§¤í•‘ ë¡œì§
    CUST_CDê°€ ì±„ë„ë§ˆìŠ¤í„°ì˜ SAP_CDì— ìˆìœ¼ë©´ RF ë°˜í™˜
    ì—†ìœ¼ë©´ ì±„ë„ì½”ë“œì— í•´ë‹¹í•˜ëŠ” ì±„ë„ëª… ë°˜í™˜
    """
    cust_cd = str(row['ê³ ê°ì½”ë“œ']).strip()
    chnl_cd = str(row['ì±„ë„ì½”ë“œ']).strip()
    
    # RF ì²´í¬: CUST_CDê°€ SAP_CDì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    # SAP_CDëŠ” ìˆ«ìì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ NULLì´ ì•„ë‹Œ ê°’ë§Œ í•„í„°ë§í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜
    rf_sap_codes = channel_master[channel_master['êµ¬ë¶„'] == 'RF']['SAP_CD'].dropna()
    rf_sap_codes_str = [str(int(float(code))).strip() for code in rf_sap_codes]
    
    if cust_cd in rf_sap_codes_str:
        return 'RF'
    
    # ì±„ë„ì½”ë“œë¡œ ì±„ë„ëª… ì°¾ê¸°
    channel_row = channel_master[channel_master['ì±„ë„ë²ˆí˜¸'].astype(str) == chnl_cd]
    if not channel_row.empty:
        return str(channel_row.iloc[0]['ì±„ë„ëª…']).strip()
    
    return 'ê¸°íƒ€'

def prepare_item_master_for_merge(item_master):
    """
    ì•„ì´í…œ ë§ˆìŠ¤í„°ë¥¼ mergeìš©ìœ¼ë¡œ ì¤€ë¹„
    PH01-3ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ê³  í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    """
    # PH01-3ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì¤‘ë³µ ì œê±°
    item_master_clean = item_master.copy()
    item_master_clean['PH01-3'] = item_master_clean['PH01-3'].astype(str).str.strip()
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ê°’ ìœ ì§€)
    item_master_clean = item_master_clean[['PH01-3', 'PRDT_HRRC2_NM', 'PRDT_HRRC3_NM']].drop_duplicates(subset=['PH01-3'], keep='first')
    
    # NM ì»¬ëŸ¼ë„ ë¬¸ìì—´ë¡œ ì •ë¦¬
    item_master_clean['PRDT_HRRC2_NM'] = item_master_clean['PRDT_HRRC2_NM'].astype(str).str.strip()
    item_master_clean['PRDT_HRRC3_NM'] = item_master_clean['PRDT_HRRC3_NM'].astype(str).str.strip()
    
    return item_master_clean

def determine_season_category(row, current_date_str: str):
    """
    ì‹œì¦Œ ë¡œì§ì„ ë°˜ì˜í•œ ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ ê³„ì‚°
    
    ì‹œì¦Œ ë¡œì§:
    - SSì‹œì¦Œ: 3ì›”~8ì›”, FWì‹œì¦Œ: 9ì›”~ìµë…„ 2ì›”
    - í˜„ì¬ ì‹œì¦Œ: ë‹¹ì‹œì¦Œì˜ë¥˜
    - ê³¼ê±° ì‹œì¦Œ: ê³¼ì‹œì¦Œì˜ë¥˜
    - ë¯¸ë˜ ì‹œì¦Œ: ì°¨ì‹œì¦Œì˜ë¥˜
    - ACC: PRDT_HRRC2_NM ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    Args:
        row: ë°ì´í„° í–‰
        current_date_str: í˜„ì¬ ë‚ ì§œ (YYYYMMDD)
    
    Returns:
        str: ì•„ì´í…œ_ì¤‘ë¶„ë¥˜
    """
    prdt_hrrc_cd1 = str(row['prdt_hrrc_cd1']).strip().upper()
    season_code = str(row['ì‹œì¦Œ']).strip().upper()
    prdt_hrrc2_nm = str(row['PRDT_HRRC2_NM']).strip()
    
    # ACCì¸ ê²½ìš° PRDT_HRRC2_NM ë°˜í™˜
    if prdt_hrrc_cd1 == 'ACC' or prdt_hrrc_cd1.startswith('E02'):
        return prdt_hrrc2_nm
    
    # ì˜ë¥˜ê°€ ì•„ë‹Œ ê²½ìš° PRDT_HRRC2_NM ë°˜í™˜
    if not (prdt_hrrc_cd1 == 'ì˜ë¥˜' or prdt_hrrc_cd1.startswith('E01') or prdt_hrrc_cd1 == 'L0100'):
        return prdt_hrrc2_nm
    
    # í˜„ì¬ ë‚ ì§œì—ì„œ ë…„/ì›” ì¶”ì¶œ
    current_year = int(current_date_str[:4])
    current_month = int(current_date_str[4:6])
    
    # í˜„ì¬ ì‹œì¦Œ ê²°ì • (SS: 3-8ì›”, FW: 9-2ì›”)
    if 3 <= current_month <= 8:
        current_season = 'S'
        current_season_year = current_year % 100  # 2025 -> 25
    else:
        current_season = 'F'
        # FW ì‹œì¦Œì€ 9ì›”~ìµë…„ 2ì›”ì´ë¯€ë¡œ
        # 1-2ì›”ì´ë©´ ì „ë…„ë„ FW ì‹œì¦Œ
        if current_month <= 2:
            current_season_year = (current_year - 1) % 100
        else:
            current_season_year = current_year % 100
    
    # ì‹œì¦Œ ì½”ë“œ íŒŒì‹± (ì˜ˆ: 25F, 25S, 25N)
    if not season_code or len(season_code) < 2:
        return prdt_hrrc2_nm
    
    try:
        # Nì„ í¬í•¨í•˜ëŠ” ê²½ìš° (ì˜ˆ: 25N) - ë…„ë„ë§Œ ë¹„êµ
        if 'N' in season_code:
            season_year = int(season_code.replace('N', ''))
            if season_year == current_season_year:
                return 'ë‹¹ì‹œì¦Œì˜ë¥˜'
            elif season_year < current_season_year:
                return 'ê³¼ì‹œì¦Œì˜ë¥˜'
            else:
                return 'ì°¨ì‹œì¦Œì˜ë¥˜'
        
        # ì¼ë°˜ ì‹œì¦Œ ì½”ë“œ (ì˜ˆ: 25F, 25S)
        season_year = int(season_code[:-1])
        season_type = season_code[-1]
        
        # ì‹œì¦Œ ë¹„êµ
        if season_year == current_season_year and season_type == current_season:
            return 'ë‹¹ì‹œì¦Œì˜ë¥˜'
        elif season_year < current_season_year:
            return 'ê³¼ì‹œì¦Œì˜ë¥˜'
        elif season_year == current_season_year:
            # ê°™ì€ ë…„ë„ì§€ë§Œ ì‹œì¦Œì´ ë‹¤ë¥¸ ê²½ìš°
            if current_season == 'F' and season_type == 'S':
                return 'ê³¼ì‹œì¦Œì˜ë¥˜'
            else:
                return 'ì°¨ì‹œì¦Œì˜ë¥˜'
        else:
            return 'ì°¨ì‹œì¦Œì˜ë¥˜'
            
    except (ValueError, IndexError):
        return prdt_hrrc2_nm

def preprocess_treemap_data(df: pd.DataFrame, current_date_str: str) -> pd.DataFrame:
    """
    ì „ë…„ ë°ì´í„° ì „ì²˜ë¦¬ (ì§‘ê³„ ìš°ì„  ìˆœì„œë¡œ ì„±ëŠ¥ ìµœì í™”)
    
    ì „ì²˜ë¦¬ ë¡œì§:
    1. ë¨¼ì € ì§‘ê³„ (í–‰ ìˆ˜ ëŒ€í­ ê°ì†Œ)
    2. ì±„ë„ì½”ë“œ -> ì±„ë„ëª… ë§¤í•‘ (RF ì²˜ë¦¬ í¬í•¨)
    3. ì•„ì´í…œ ì†Œë¶„ë¥˜ -> PRDT_HRRC2_NM, PRDT_HRRC3_NM ë§¤í•‘
    4. ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ í•„ë“œ ì¶”ê°€ (ì‹œì¦Œ ë¡œì§ ë°˜ì˜)
    5. ìµœì¢… ì¶œë ¥ í˜•ì‹ ì§‘ê³„
    
    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        current_date_str: í˜„ì¬ ë‚ ì§œ (YYYYMMDD) - ì‹œì¦Œ íŒë‹¨ìš©
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°
    """
    print("\n[ì „ì²˜ë¦¬] ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # ë§ˆìŠ¤í„° ë¡œë“œ
    channel_master = load_channel_master()
    item_master = load_item_master()
    
    print(f"  ì›ë³¸ ë°ì´í„°: {len(df):,}ê±´")
    
    # 1) ë¨¼ì € ì§‘ê³„ (ì„±ëŠ¥ ìµœì í™”)
    print("  [1/5] ë°ì´í„° ì§‘ê³„ ì¤‘...")
    # ìˆ«ì ë³€í™˜
    df['TAGë§¤ì¶œ'] = pd.to_numeric(df['TAGë§¤ì¶œ'], errors='coerce').fillna(0)
    df['ì‹¤íŒë§¤ì¶œ'] = pd.to_numeric(df['ì‹¤íŒë§¤ì¶œ'], errors='coerce').fillna(0)
    
    # ì§‘ê³„ í‚¤: ë¸Œëœë“œì½”ë“œ, ì‹œì¦Œ, ì±„ë„ì½”ë“œ, ê³ ê°ì½”ë“œ, prdt_hrrc_cd1, prdt_hrrc_cd2, prdt_hrrc_cd3, ì•„ì´í…œì½”ë“œ
    group_cols = ['ë¸Œëœë“œì½”ë“œ', 'ì‹œì¦Œ', 'ì±„ë„ì½”ë“œ', 'ê³ ê°ì½”ë“œ', 'prdt_hrrc_cd1', 'prdt_hrrc_cd2', 'prdt_hrrc_cd3', 'ì•„ì´í…œì½”ë“œ']
    
    df_agg = df.groupby(group_cols, as_index=False).agg({
        'TAGë§¤ì¶œ': 'sum',
        'ì‹¤íŒë§¤ì¶œ': 'sum'
    })
    
    print(f"    ì§‘ê³„ í›„: {len(df):,}ê±´ â†’ {len(df_agg):,}ê±´ (ê°ì†Œìœ¨: {(1 - len(df_agg) / len(df)) * 100:.1f}%)")
    
    # 2) ì±„ë„ëª… ë§¤í•‘
    print("  [2/5] ì±„ë„ëª… ë§¤í•‘ ì¤‘...")
    df_agg['ì±„ë„ëª…'] = df_agg.apply(lambda row: map_channel_name(row, channel_master), axis=1)
    rf_count = (df_agg['ì±„ë„ëª…'] == 'RF').sum()
    print(f"    RF ë§¤í•‘: {rf_count:,}ê±´")
    
    # 3) ì•„ì´í…œ ì •ë³´ ë§¤í•‘ (PRDT_HRRC2_NM, PRDT_HRRC3_NM) - merge ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ 
    print("  [3/5] ì•„ì´í…œ ì •ë³´ ë§¤í•‘ ì¤‘...")
    # prdt_hrrc_cd3ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    df_agg['prdt_hrrc_cd3_str'] = df_agg['prdt_hrrc_cd3'].astype(str).str.strip()
    
    # ì•„ì´í…œ ë§ˆìŠ¤í„° ì¤€ë¹„
    item_master_clean = prepare_item_master_for_merge(item_master)
    
    # mergeë¡œ ë§¤í•‘ (left join)
    df_agg = df_agg.merge(
        item_master_clean,
        left_on='prdt_hrrc_cd3_str',
        right_on='PH01-3',
        how='left'
    )
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° 'ê¸°íƒ€'ë¡œ ì±„ìš°ê¸°
    df_agg['PRDT_HRRC2_NM'] = df_agg['PRDT_HRRC2_NM'].fillna('ê¸°íƒ€')
    df_agg['PRDT_HRRC3_NM'] = df_agg['PRDT_HRRC3_NM'].fillna('ê¸°íƒ€')
    
    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df_agg.drop(columns=['prdt_hrrc_cd3_str', 'PH01-3'], inplace=True, errors='ignore')
    
    # 4) ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ í•„ë“œ ì¶”ê°€ (ì‹œì¦Œ ë¡œì§)
    print("  [4/5] ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ ê³„ì‚° ì¤‘ (ì‹œì¦Œ ë¡œì§ ì ìš©)...")
    df_agg['ì•„ì´í…œ_ì¤‘ë¶„ë¥˜'] = df_agg.apply(lambda row: determine_season_category(row, current_date_str), axis=1)
    
    # ë‹¹/ê³¼/ì°¨ì‹œì¦Œ í†µê³„
    season_counts = df_agg['ì•„ì´í…œ_ì¤‘ë¶„ë¥˜'].value_counts()
    for season, count in season_counts.items():
        if 'ì‹œì¦Œ' in season:
            print(f"    {season}: {count:,}ê±´")
    
    # 5) ìµœì¢… ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬
    print("  [5/5] ìµœì¢… ë°ì´í„° ì •ë¦¬ ì¤‘...")
    
    # ë¸Œëœë“œ = ë¸Œëœë“œì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€í™˜í•˜ì§€ ì•ŠìŒ)
    df_agg['ë¸Œëœë“œ'] = df_agg['ë¸Œëœë“œì½”ë“œ']
    
    # ìœ í†µì±„ë„ = ì±„ë„ì½”ë“œ (ì›ë³¸ ê°’ ìœ ì§€)
    df_agg['ìœ í†µì±„ë„'] = df_agg['ì±„ë„ì½”ë“œ']
    
    # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
    output_columns = [
        'ë¸Œëœë“œì½”ë“œ',
        'ì‹œì¦Œ',
        'ì±„ë„ì½”ë“œ',
        'ê³ ê°ì½”ë“œ',
        'prdt_hrrc_cd1',
        'prdt_hrrc_cd2',
        'PRDT_HRRC2_NM',
        'prdt_hrrc_cd3',
        'PRDT_HRRC3_NM',
        'ì•„ì´í…œì½”ë“œ',
        'TAGë§¤ì¶œ',
        'ì‹¤íŒë§¤ì¶œ',
        'ë¸Œëœë“œ',
        'ìœ í†µì±„ë„',
        'ì±„ë„ëª…',
        'ì•„ì´í…œ_ì¤‘ë¶„ë¥˜'
    ]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_columns = [col for col in output_columns if col in df_agg.columns]
    result_df = df_agg[available_columns].copy()
    
    print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(result_df):,}ê±´")
    print(f"  ë¸Œëœë“œ ìˆ˜: {result_df['ë¸Œëœë“œì½”ë“œ'].nunique()}ê°œ")
    print(f"  ì±„ë„ ìˆ˜: {result_df['ì±„ë„ëª…'].nunique()}ê°œ")
    print(f"  ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ ìˆ˜: {result_df['ì•„ì´í…œ_ì¤‘ë¶„ë¥˜'].nunique()}ê°œ")
    
    return result_df

def save_to_csv(df: pd.DataFrame, output_path: Path):
    """DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥ (ì „ì²˜ë¦¬ ì™„ë£Œ ë²„ì „)"""
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ìµœì¢… ì¶œë ¥ í˜•ì‹ (ìš”ì²­ì‚¬í•­):
        # í–‰: ë¸Œëœë“œ, ìœ í†µì±„ë„, ì±„ë„ëª…, ì•„ì´í…œ_ì¤‘ë¶„ë¥˜, ì•„ì´í…œì†Œë¶„ë¥˜, ì•„ì´í…œì½”ë“œ
        # ê°’: íŒë§¤ê¸ˆì•¡(TAGê°€), ì‹¤íŒë§¤ì•¡
        
        # ê·¸ë£¹í•‘ ì§‘ê³„
        group_columns = ['ë¸Œëœë“œ', 'ìœ í†µì±„ë„', 'ì±„ë„ëª…', 'ì•„ì´í…œ_ì¤‘ë¶„ë¥˜', 'PRDT_HRRC3_NM', 'ì•„ì´í…œì½”ë“œ']
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        available_group_cols = [col for col in group_columns if col in df.columns]
        
        df_aggregated = df.groupby(available_group_cols, as_index=False).agg({
            'TAGë§¤ì¶œ': 'sum',
            'ì‹¤íŒë§¤ì¶œ': 'sum'
        })
        
        # ì»¬ëŸ¼ëª… ë³€ê²½ (ìš”ì²­ì‚¬í•­ì— ë§ì¶¤)
        df_aggregated = df_aggregated.rename(columns={
            'PRDT_HRRC3_NM': 'ì•„ì´í…œì†Œë¶„ë¥˜',
            'TAGë§¤ì¶œ': 'íŒë§¤ê¸ˆì•¡(TAGê°€)',
            'ì‹¤íŒë§¤ì¶œ': 'ì‹¤íŒë§¤ì•¡'
        })
        
        # ìµœì¢… ì¶œë ¥ ì»¬ëŸ¼ (í• ì¸ìœ¨ ì œì™¸)
        final_columns = ['ë¸Œëœë“œ', 'ìœ í†µì±„ë„', 'ì±„ë„ëª…', 'ì•„ì´í…œ_ì¤‘ë¶„ë¥˜', 'ì•„ì´í…œì†Œë¶„ë¥˜', 'ì•„ì´í…œì½”ë“œ', 'íŒë§¤ê¸ˆì•¡(TAGê°€)', 'ì‹¤íŒë§¤ì•¡']
        available_final_cols = [col for col in final_columns if col in df_aggregated.columns]
        
        df_output = df_aggregated[available_final_cols]
        df_output.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {output_path.stat().st_size / 1024:.2f} KB")
        print(f"   ë°ì´í„° í–‰ ìˆ˜: {len(df_output):,}ê±´")
        print(f"   ì¶œë ¥ ì»¬ëŸ¼: {', '.join(available_final_cols)}")
        
        # ìš”ì•½ í†µê³„
        total_tag = df_output['íŒë§¤ê¸ˆì•¡(TAGê°€)'].sum()
        total_sales = df_output['ì‹¤íŒë§¤ì•¡'].sum()
        print(f"   ì´ íŒë§¤ê¸ˆì•¡(TAGê°€): {total_tag / 100000000:.1f}ì–µì›")
        print(f"   ì´ ì‹¤íŒë§¤ì•¡: {total_sales / 100000000:.1f}ì–µì›")
        
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        raise

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='íŠ¸ë¦¬ë§µ ì „ë…„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬')
    parser.add_argument('update_date', help='ì—…ë°ì´íŠ¸ì¼ì (YYYYMMDD í˜•ì‹, ì˜ˆ: 20251215)')
    parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    
    args = parser.parse_args()
    update_date = args.update_date
    
    if len(update_date) != 8 or not update_date.isdigit():
        print("[ERROR] ì—…ë°ì´íŠ¸ì¼ì í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYYMMDD í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        return 1
    
    conn = None
    
    try:
        print("=" * 70)
        print("íŠ¸ë¦¬ë§µ ì „ë…„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬")
        print("=" * 70)
        print(f"ì—…ë°ì´íŠ¸ì¼ì: {update_date}")
        print()
        
        # ì „ë…„ ê¸°ê°„ ê³„ì‚°
        prev_start, prev_end = calculate_previous_year_period(update_date)
        print()
        
        # Snowflake ì—°ê²°
        conn = get_snowflake_connection()
        print()
        
        # ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰
        query = get_treemap_previous_year_query(prev_start, prev_end)
        df = execute_query_to_dataframe(conn, query)
        
        # ë°ì´í„° ì „ì²˜ë¦¬ (ë§ˆìŠ¤í„° ë§¤í•‘ + ì‹œì¦Œ ë¡œì§ + ìµœì¢… í˜•ì‹)
        # ì „ë…„ ë°ì´í„°ì´ë¯€ë¡œ ì‹œì¦Œ íŒë‹¨ë„ ì „ë…„ ì¢…ë£Œì¼ ê¸°ì¤€ìœ¼ë¡œ
        # prev_endëŠ” 'YYYY-MM-DD' í˜•ì‹ì´ë¯€ë¡œ YYYYMMDDë¡œ ë³€í™˜
        prev_end_yyyymmdd = prev_end.replace('-', '')
        df_processed = preprocess_treemap_data(df, prev_end_yyyymmdd)
        
        # ì¶œë ¥ ê²½ë¡œ ê²°ì •
        if args.output:
            output_path = Path(args.output)
        else:
            year_month = update_date[:6]
            # íŒŒì¼ëª…: treemap_preprocessed_prev_YYYYMMDD.csv (ì „ì²˜ë¦¬ ì™„ë£Œ ë²„ì „)
            output_path = project_root / "raw" / year_month / "previous_year" / f"treemap_preprocessed_prev_{update_date}.csv"
        
        # CSV ì €ì¥
        save_to_csv(df_processed, output_path)
        
        # ë°ì´í„° ìš”ì•½
        print()
        print("=" * 70)
        print("ğŸ“Š ë°ì´í„° ìš”ì•½")
        print("=" * 70)
        print(f"ì´ í–‰ ìˆ˜: {len(df_processed):,}ê±´")
        
        if 'ë¸Œëœë“œ' in df_processed.columns:
            print(f"ë¸Œëœë“œ ìˆ˜: {df_processed['ë¸Œëœë“œ'].nunique()}ê°œ")
        if 'ì±„ë„ëª…' in df_processed.columns:
            print(f"ì±„ë„ ìˆ˜: {df_processed['ì±„ë„ëª…'].nunique()}ê°œ")
        if 'ì•„ì´í…œ_ì¤‘ë¶„ë¥˜' in df_processed.columns:
            print(f"ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ ìˆ˜: {df_processed['ì•„ì´í…œ_ì¤‘ë¶„ë¥˜'].nunique()}ê°œ")
            
            # ì•„ì´í…œ_ì¤‘ë¶„ë¥˜ë³„ í†µê³„
            print("\nì•„ì´í…œ_ì¤‘ë¶„ë¥˜ë³„ í†µê³„:")
            item_cat_summary = df_processed.groupby('ì•„ì´í…œ_ì¤‘ë¶„ë¥˜')['ì‹¤íŒë§¤ì¶œ'].sum().sort_values(ascending=False)
            for cat, sales in item_cat_summary.items():
                print(f"  {cat}: {sales / 100000000:.1f}ì–µì›")
        
        print(f"\nì „ë…„ TAGë§¤ì¶œ í•©ê³„: {df_processed['TAGë§¤ì¶œ'].sum() / 100000000:.1f}ì–µì›")
        print(f"ì „ë…„ ì‹¤íŒë§¤ì¶œ í•©ê³„: {df_processed['ì‹¤íŒë§¤ì¶œ'].sum() / 100000000:.1f}ì–µì›")
        
        # ë¸Œëœë“œë³„ ìš”ì•½
        if 'ë¸Œëœë“œ' in df_processed.columns:
            print("\në¸Œëœë“œë³„ ì‹¤íŒë§¤ì¶œ:")
            brand_summary = df_processed.groupby('ë¸Œëœë“œ')['ì‹¤íŒë§¤ì¶œ'].sum().sort_values(ascending=False)
            for brand, sales in brand_summary.items():
                print(f"  {brand}: {sales / 100000000:.1f}ì–µì›")
        
        # ì±„ë„ë³„ ìš”ì•½ (ìƒìœ„ 5ê°œ)
        if 'ì±„ë„ëª…' in df_processed.columns:
            print("\nì±„ë„ë³„ ì‹¤íŒë§¤ì¶œ (ìƒìœ„ 5ê°œ):")
            channel_summary = df_processed.groupby('ì±„ë„ëª…')['ì‹¤íŒë§¤ì¶œ'].sum().sort_values(ascending=False).head(5)
            for channel, sales in channel_summary.items():
                print(f"  {channel}: {sales / 100000000:.1f}ì–µì›")
        
        print()
        print("=" * 70)
        print("âœ… ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 70)
        
        return 0
        
    except Exception as e:
        print()
        print("=" * 70)
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("=" * 70)
        import traceback
        traceback.print_exc()
        return 1
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ Snowflake ì—°ê²° ì¢…ë£Œ")

if __name__ == "__main__":
    import sys
    sys.exit(main())

