"""
ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ ë°ì´í„°ë¥¼ Snowflakeì—ì„œ ì¡°íšŒí•˜ì—¬ CSVë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/download_weekly_sales_trend.py [ì—…ë°ì´íŠ¸ì¼ì]
    
ì˜ˆì‹œ:
    python scripts/download_weekly_sales_trend.py 2025-11-24
    python scripts/download_weekly_sales_trend.py  # ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
    
ì„¤ëª…:
    ì—…ë°ì´íŠ¸ì¼ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ì£¼ì°¨ê¹Œì§€ì˜ 9ì£¼ì¹˜ ë§¤ì¶œ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    - ë‹¹ë…„ ë§¤ì¶œê³¼ ì „ë…„ ë™ì£¼ì°¨ ë§¤ì¶œì„ ëª¨ë‘ ì¡°íšŒ
    - Xì¶•ì—ëŠ” ì£¼ì°¨ì¢…ë£Œ ì¼ìš”ì¼ ë‚ ì§œ í‘œì‹œ
    - YOY(ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥ ) ê³„ì‚°
    
í™˜ê²½ ë³€ìˆ˜:
    SNOWFLAKE_ACCOUNT: Snowflake ê³„ì •ëª…
    SNOWFLAKE_USERNAME: ì‚¬ìš©ìëª…
    SNOWFLAKE_PASSWORD: ë¹„ë°€ë²ˆí˜¸
    SNOWFLAKE_WAREHOUSE: ì›¨ì–´í•˜ìš°ìŠ¤ëª…
    SNOWFLAKE_DATABASE: ë°ì´í„°ë² ì´ìŠ¤ëª…
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
import snowflake.connector
import pandas as pd
import warnings

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# path_utils ì„í¬íŠ¸
from scripts.path_utils import get_plan_file_path, extract_year_month_from_date

# .env íŒŒì¼ ë¡œë“œ
env_path = project_root / '.env'
if env_path.exists():
    load_dotenv(env_path)
else:
    print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì½ìŠµë‹ˆë‹¤.")


def get_snowflake_connection():
    """
    Snowflake ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±
    
    Returns:
        snowflake.connector.SnowflakeConnection: Snowflake ì—°ê²° ê°ì²´
    """
    try:
        conn = snowflake.connector.connect(
            account=os.getenv('SNOWFLAKE_ACCOUNT'),
            user=os.getenv('SNOWFLAKE_USERNAME'),
            password=os.getenv('SNOWFLAKE_PASSWORD'),
            warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
            database=os.getenv('SNOWFLAKE_DATABASE'),
            network_timeout=None,  # íƒ€ì„ì•„ì›ƒ ì—†ìŒ
            login_timeout=60,      # ë¡œê·¸ì¸ 1ë¶„ íƒ€ì„ì•„ì›ƒ
            session_parameters={
                'QUERY_TAG': 'weekly_sales_trend',
                'STATEMENT_TIMEOUT_IN_SECONDS': 3600  # ì¿¼ë¦¬ 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            }
        )
        print("âœ… Snowflake ì—°ê²° ì„±ê³µ!")
        return conn
    except Exception as e:
        print(f"âŒ Snowflake ì—°ê²° ì‹¤íŒ¨: {e}")
        raise


def load_channel_master() -> dict:
    """
    ì±„ë„ë§ˆìŠ¤í„° íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ìœ í†µì±„ë„ ì½”ë“œ â†’ ì±„ë„ëª… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    
    Returns:
        dict: {ìœ í†µì±„ë„ì½”ë“œ: ì±„ë„ëª…} ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    """
    master_path = project_root / "Master" / "ì±„ë„ë§ˆìŠ¤í„°.csv"
    
    # ê¸°ë³¸ ë§¤í•‘ (íŒŒì¼ì´ ì—†ì„ ê²½ìš° ì‚¬ìš©)
    default_mapping = {
        'RF': 'RF',
        '01': 'ë°±í™”ì ',
        '02': 'ë©´ì„¸ì ',
        '03': 'ì§ì˜ì ',
        '04': 'ìì‚¬ëª°',
        '05': 'ì œíœ´ëª°',
        '06': 'ëŒ€ë¦¬ì ',
        '07': 'ì•„ìš¸ë ›',
        '08': 'ì‚¬ì…',
        '09': 'ìˆ˜ì¶œ',
        '11': 'ì§ì˜ëª°',
        '12': 'ì•„ìš¸ë ›',
        '99': 'ê¸°íƒ€'
    }
    
    if not master_path.exists():
        print(f"âš ï¸ ì±„ë„ë§ˆìŠ¤í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {master_path}")
        print("   ê¸°ë³¸ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_mapping
    
    try:
        master_df = pd.read_csv(master_path, encoding='utf-8-sig')
        
        # ì±„ë„ë²ˆí˜¸ â†’ ì±„ë„ëª… ë§¤í•‘ ìƒì„±
        channel_mapping = {}
        for _, row in master_df.iterrows():
            channel_code = str(row['ì±„ë„ë²ˆí˜¸']).strip()
            channel_name = str(row['ì±„ë„ëª…']).strip()
            
            # ìˆ«ìì¸ ê²½ìš° ì•ì— 0 ë¶™ì´ê¸° (1 â†’ 01)
            if channel_code.isdigit() and len(channel_code) == 1:
                channel_code = f"0{channel_code}"
            
            channel_mapping[channel_code] = channel_name
        
        print(f"âœ… ì±„ë„ë§ˆìŠ¤í„° ë¡œë“œ ì™„ë£Œ: {len(channel_mapping)}ê°œ ì±„ë„")
        return channel_mapping
    
    except Exception as e:
        print(f"âš ï¸ ì±„ë„ë§ˆìŠ¤í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   ê¸°ë³¸ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_mapping


def map_channel_name(df: pd.DataFrame, channel_mapping: dict) -> pd.DataFrame:
    """
    ìœ í†µì±„ë„ ì½”ë“œë¥¼ ì±„ë„ëª…ìœ¼ë¡œ ë§¤í•‘
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„ (ìœ í†µì±„ë„ ì»¬ëŸ¼ í¬í•¨)
        channel_mapping: ì±„ë„ì½”ë“œ â†’ ì±„ë„ëª… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        pd.DataFrame: ì±„ë„ëª… ì»¬ëŸ¼ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    df = df.copy()
    df['ì±„ë„ëª…'] = df['ìœ í†µì±„ë„'].map(channel_mapping).fillna('ê¸°íƒ€')
    return df


def calculate_week_end_dates(update_date: datetime, weeks: int = 9) -> tuple:
    """
    ì—…ë°ì´íŠ¸ì¼ì ê¸°ì¤€ìœ¼ë¡œ ì´ì „ 9ì£¼ì°¨ì˜ ì£¼ì°¨ ì¢…ë£Œì¼(ì¼ìš”ì¼) ëª©ë¡ ê³„ì‚°
    
    ì—…ë°ì´íŠ¸ì¼ìê°€ ì›”ìš”ì¼ì¸ ê²½ìš°:
    - ë‹¹ì¼ì´ ì†í•œ ì£¼ì˜ ì§ì „ ì£¼(ì „ì£¼ì°¨)ê¹Œì§€ ë¶„ì„
    - ì˜ˆ: 2025-11-24(ì›”) â†’ 11/23ê¹Œì§€ ë¶„ì„ (11/17~11/23 ì£¼ì°¨ í¬í•¨)
    
    Args:
        update_date: ì—…ë°ì´íŠ¸ì¼ì (ì˜ˆ: 2025-11-24)
        weeks: ê°€ì ¸ì˜¬ ì£¼ì°¨ ìˆ˜ (ê¸°ë³¸ 9ì£¼)
    
    Returns:
        tuple: (ì£¼ì°¨ì¢…ë£Œì¼ ë¦¬ìŠ¤íŠ¸, ì‹œì‘ì¼, ì¢…ë£Œì¼)
    """
    # ì—…ë°ì´íŠ¸ì¼ìì˜ ìš”ì¼ í™•ì¸ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
    weekday = update_date.weekday()
    
    # ë§ˆì§€ë§‰ ë¶„ì„ ëŒ€ìƒ ì£¼ì°¨ì˜ ì¼ìš”ì¼ ì°¾ê¸°
    # ì—…ë°ì´íŠ¸ì¼ìê°€ ì›”ìš”ì¼(0)ì´ë©´, ì „ì£¼ ì¼ìš”ì¼ì´ ë§ˆì§€ë§‰ ë¶„ì„ ì£¼ì˜ ì¢…ë£Œì¼
    if weekday == 0:  # ì›”ìš”ì¼
        # ì „ì£¼ ì¼ìš”ì¼ = ì—…ë°ì´íŠ¸ì¼ì - 1ì¼
        last_sunday = update_date - timedelta(days=1)
    else:
        # ê·¸ ì™¸ì˜ ê²½ìš°, ì´ì „ ì™„ë£Œëœ ì£¼ì˜ ì¼ìš”ì¼ ì°¾ê¸°
        days_since_sunday = (weekday + 1) % 7
        last_sunday = update_date - timedelta(days=days_since_sunday + 7)
    
    # 9ì£¼ì°¨ì˜ ì¢…ë£Œì¼(ì¼ìš”ì¼) ë¦¬ìŠ¤íŠ¸ ìƒì„± (ìµœì‹  ìˆœ)
    week_end_dates = []
    for i in range(weeks):
        end_date = last_sunday - timedelta(days=7 * i)
        week_end_dates.append(end_date)
    
    # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì •ë ¬
    week_end_dates.sort()
    
    # ì‹œì‘ì¼ = ê°€ì¥ ì˜¤ë˜ëœ ì£¼ì˜ ì›”ìš”ì¼ (ì¼ìš”ì¼ - 6ì¼)
    start_date = week_end_dates[0] - timedelta(days=6)
    
    # ì¢…ë£Œì¼ = ê°€ì¥ ìµœì‹  ì£¼ì˜ ì¼ìš”ì¼
    end_date = week_end_dates[-1]
    
    return week_end_dates, start_date, end_date


def get_weekly_sales_query(start_date: datetime, end_date: datetime) -> str:
    """
    ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ ì¡°íšŒ ì¿¼ë¦¬ ìƒì„±
    
    Args:
        start_date: ì¡°íšŒ ì‹œì‘ì¼ (ì²« ì£¼ ì›”ìš”ì¼)
        end_date: ì¡°íšŒ ì¢…ë£Œì¼ (ë§ˆì§€ë§‰ ì£¼ ì¼ìš”ì¼)
    
    Returns:
        str: SQL ì¿¼ë¦¬
    """
    start_str = start_date.strftime('%Y-%m-%d')
    end_str = end_date.strftime('%Y-%m-%d')
    
    query = f"""
WITH weeks AS (
    SELECT DISTINCT END_DT
    FROM FNF.PRCS.DB_SH_S_W
    WHERE END_DT BETWEEN '{start_str}'::DATE AND '{end_str}'::DATE
),

curr AS (  -- ë‹¹ë…„
    SELECT
        s.BRD_CD AS "ë¸Œëœë“œ",
        w.END_DT AS "ì¢…ë£Œì¼",
        CASE
            WHEN s.BRD_CD = 'M'
             AND s.SHOP_ID IN ('649','155','524','526','82','744','6048','954')
                THEN 'RF'
            ELSE sh.DIST_TYPE_SAP
        END AS "ìœ í†µì±„ë„",
        SUM(
            CASE
                WHEN sh.DIST_TYPE_SAP IN ('08','99')
                    THEN (s.DELV_NML_SUPP_AMT + s.DELV_RET_SUPP_AMT)
                ELSE (s.SALE_NML_SALE_AMT + s.SALE_RET_SALE_AMT)
            END
        ) AS "ì‹¤íŒë§¤ì¶œ"
    FROM weeks w
    JOIN FNF.PRCS.DB_SH_S_W s
      ON s.END_DT = w.END_DT
    JOIN FNF.PRCS.DB_SHOP sh
      ON sh.BRD_CD = s.BRD_CD
     AND sh.SHOP_ID = s.SHOP_ID
     AND sh.ANAL_CNTRY = 'KO'
    WHERE s.BRD_CD != 'A'
    GROUP BY 1,2,3
    HAVING SUM(
            CASE
                WHEN sh.DIST_TYPE_SAP IN ('08','99')
                    THEN (s.DELV_NML_SUPP_AMT + s.DELV_RET_SUPP_AMT)
                ELSE (s.SALE_NML_SALE_AMT + s.SALE_RET_SALE_AMT)
            END
        ) <> 0
),

prev AS (  -- ì „ë…„ ë™ì£¼ì°¨
    SELECT
        s.BRD_CD AS "ë¸Œëœë“œ",
        w.END_DT AS "ì¢…ë£Œì¼",
        CASE
            WHEN s.BRD_CD = 'M'
             AND s.SHOP_ID IN ('649','155','524','526','82','744','6048','954')
                THEN 'RF'
            ELSE sh.DIST_TYPE_SAP
        END AS "ìœ í†µì±„ë„",
        SUM(
            CASE
                WHEN sh.DIST_TYPE_SAP IN ('08','99')
                    THEN (s.DELV_NML_SUPP_AMT + s.DELV_RET_SUPP_AMT)
                ELSE (s.SALE_NML_SALE_AMT + s.SALE_RET_SALE_AMT)
            END
        ) AS "ì‹¤íŒë§¤ì¶œ"
    FROM weeks w
    JOIN FNF.PRCS.DB_SH_S_W s
      ON s.END_DT = DATE_TRUNC('WEEK', DATEADD(YEAR, -1, w.END_DT)) + 6
    JOIN FNF.PRCS.DB_SHOP sh
      ON sh.BRD_CD = s.BRD_CD
     AND sh.SHOP_ID = s.SHOP_ID
     AND sh.ANAL_CNTRY = 'KO'
    WHERE s.BRD_CD != 'A'
    GROUP BY 1,2,3
    HAVING SUM(
            CASE
                WHEN sh.DIST_TYPE_SAP IN ('08','99')
                    THEN (s.DELV_NML_SUPP_AMT + s.DELV_RET_SUPP_AMT)
                ELSE (s.SALE_NML_SALE_AMT + s.SALE_RET_SALE_AMT)
            END
        ) <> 0
)

SELECT  "ë¸Œëœë“œ",
        'ë‹¹ë…„' AS "êµ¬ë¶„",
        "ì¢…ë£Œì¼",
        "ìœ í†µì±„ë„",
        "ì‹¤íŒë§¤ì¶œ"
FROM curr
UNION ALL
SELECT  "ë¸Œëœë“œ",
        'ì „ë…„' AS "êµ¬ë¶„",
        "ì¢…ë£Œì¼",
        "ìœ í†µì±„ë„",
        "ì‹¤íŒë§¤ì¶œ"
FROM prev
ORDER BY "ì¢…ë£Œì¼", "ë¸Œëœë“œ", "ìœ í†µì±„ë„", "êµ¬ë¶„"
"""
    
    return query


def execute_query_to_dataframe(conn, query: str):
    """
    Snowflake ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ë¥¼ pandas DataFrameìœ¼ë¡œ ë°˜í™˜
    
    Args:
        conn: Snowflake ì—°ê²° ê°ì²´
        query: ì‹¤í–‰í•  SQL ì¿¼ë¦¬
        
    Returns:
        pd.DataFrame: ì¿¼ë¦¬ ê²°ê³¼
    """
    import time
    import sys
    try:
        print("ğŸ“Š ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...", flush=True)
        print("   (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹œ ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)", flush=True)
        
        cursor = conn.cursor()
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        print("   ì¿¼ë¦¬ ì „ì†¡ ì¤‘...", flush=True)
        sys.stdout.flush()  # ê°•ì œë¡œ ì¶œë ¥ ë²„í¼ ë¹„ìš°ê¸°
        
        cursor.execute(query)
        
        exec_time = time.time() - start_time
        print(f"   ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ ({exec_time:.1f}ì´ˆ)", flush=True)
        
        # ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°
        columns = [desc[0] for desc in cursor.description]
        
        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“¥ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...", flush=True)
        sys.stdout.flush()
        fetch_start = time.time()
        
        # ë°°ì¹˜ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 10000
        all_data = []
        batch_count = 0
        
        while True:
            batch = cursor.fetchmany(batch_size)
            if not batch:
                break
            all_data.extend(batch)
            batch_count += 1
            if batch_count % 10 == 0:  # 10ë§Œ ê±´ë§ˆë‹¤ ì§„í–‰ ìƒí™© í‘œì‹œ
                print(f"   ì§„í–‰ ì¤‘... {len(all_data):,}ê±´ ì¡°íšŒë¨", flush=True)
                sys.stdout.flush()
        
        fetch_time = time.time() - fetch_start
        print(f"   ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ ({fetch_time:.1f}ì´ˆ)", flush=True)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_data, columns=columns)
        
        cursor.close()
        print(f"âœ… ì´ {len(df):,}ê±´ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.", flush=True)
        print(f"   ì „ì²´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ", flush=True)
        return df
    except Exception as e:
        print(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        raise


def process_weekly_sales_data(df: pd.DataFrame, channel_mapping: dict) -> pd.DataFrame:
    """
    ì£¼ì°¨ë³„ ë§¤ì¶œ ë°ì´í„° ì²˜ë¦¬ - ì±„ë„ëª… ë§¤í•‘ ì¶”ê°€
    
    Args:
        df: ì›ì‹œ ë°ì´í„° DataFrame
        channel_mapping: ì±„ë„ì½”ë“œ â†’ ì±„ë„ëª… ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        pd.DataFrame: ì±„ë„ëª…ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("\nğŸ“ˆ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    
    # ì±„ë„ëª… ë§¤í•‘ ì¶”ê°€
    df = map_channel_name(df, channel_mapping)
    
    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬: ë¸Œëœë“œ, êµ¬ë¶„, ì¢…ë£Œì¼, ìœ í†µì±„ë„, ì±„ë„ëª…, ì‹¤íŒë§¤ì¶œ
    df = df[['ë¸Œëœë“œ', 'êµ¬ë¶„', 'ì¢…ë£Œì¼', 'ìœ í†µì±„ë„', 'ì±„ë„ëª…', 'ì‹¤íŒë§¤ì¶œ']]
    
    # ì •ë ¬: ì¢…ë£Œì¼, ë¸Œëœë“œ, ìœ í†µì±„ë„, êµ¬ë¶„(ë‹¹ë…„ ë¨¼ì €)
    df['êµ¬ë¶„ì •ë ¬'] = df['êµ¬ë¶„'].map({'ë‹¹ë…„': 0, 'ì „ë…„': 1})
    df = df.sort_values(['ì¢…ë£Œì¼', 'ë¸Œëœë“œ', 'ìœ í†µì±„ë„', 'êµ¬ë¶„ì •ë ¬'])
    df = df.drop(columns=['êµ¬ë¶„ì •ë ¬'])
    
    return df


def save_to_csv(df: pd.DataFrame, output_path: Path, description: str = ""):
    """
    DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        df: ì €ì¥í•  DataFrame
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        description: íŒŒì¼ ì„¤ëª…
    """
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # CSVë¡œ ì €ì¥ (UTF-8 with BOM for Excel compatibility)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… {description} ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {output_path.stat().st_size / 1024:.2f} KB")
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def save_to_js(df: pd.DataFrame, output_path: Path, update_date: datetime, 
               week_end_dates: list, channel_mapping: dict, description: str = ""):
    """
    DataFrameì„ JavaScript íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        df: ì €ì¥í•  DataFrame
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        update_date: ì—…ë°ì´íŠ¸ ì¼ì
        week_end_dates: ì£¼ì°¨ ì¢…ë£Œì¼ ë¦¬ìŠ¤íŠ¸
        channel_mapping: ì±„ë„ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        description: íŒŒì¼ ì„¤ëª…
    """
    import json
    
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ë³€í™˜
        df_copy = df.copy()
        df_copy['ì¢…ë£Œì¼'] = df_copy['ì¢…ë£Œì¼'].astype(str)
        data_list = df_copy.to_dict('records')
        
        # ì£¼ì°¨ í‘œì‹œ ë¦¬ìŠ¤íŠ¸
        weeks = [f"{d.month}/{d.day}" for d in week_end_dates]
        
        # ë¸Œëœë“œë³„, ì±„ë„ëª…ë³„ ì§‘ê³„ ë°ì´í„° ìƒì„±
        brand_summary = {}
        for brand in df['ë¸Œëœë“œ'].unique():
            brand_df = df[df['ë¸Œëœë“œ'] == brand]
            brand_summary[brand] = {
                'weekly': {},
                'channels': {}
            }
            
            # ì£¼ì°¨ë³„ í•©ê³„
            for end_date in week_end_dates:
                date_str = str(end_date.date())
                week_label = f"{end_date.month}/{end_date.day}"
                week_df = brand_df[brand_df['ì¢…ë£Œì¼'].astype(str) == date_str]
                
                curr = week_df[week_df['êµ¬ë¶„'] == 'ë‹¹ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
                prev = week_df[week_df['êµ¬ë¶„'] == 'ì „ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
                yoy = round((curr - prev) / prev * 100, 2) if prev != 0 else 0
                
                brand_summary[brand]['weekly'][week_label] = {
                    'ë‹¹ë…„': int(curr),
                    'ì „ë…„': int(prev),
                    'YOY': yoy
                }
            
            # ì±„ë„ë³„ í•©ê³„
            for channel_name in brand_df['ì±„ë„ëª…'].unique():
                ch_df = brand_df[brand_df['ì±„ë„ëª…'] == channel_name]
                curr = ch_df[ch_df['êµ¬ë¶„'] == 'ë‹¹ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
                prev = ch_df[ch_df['êµ¬ë¶„'] == 'ì „ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
                yoy = round((curr - prev) / prev * 100, 2) if prev != 0 else 0
                
                brand_summary[brand]['channels'][channel_name] = {
                    'ë‹¹ë…„': int(curr),
                    'ì „ë…„': int(prev),
                    'YOY': yoy
                }
        
        # ì „ì²´ ì§‘ê³„
        total_summary = {'weekly': {}, 'channels': {}}
        for end_date in week_end_dates:
            date_str = str(end_date.date())
            week_label = f"{end_date.month}/{end_date.day}"
            week_df = df[df['ì¢…ë£Œì¼'].astype(str) == date_str]
            
            curr = week_df[week_df['êµ¬ë¶„'] == 'ë‹¹ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
            prev = week_df[week_df['êµ¬ë¶„'] == 'ì „ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
            yoy = round((curr - prev) / prev * 100, 2) if prev != 0 else 0
            
            total_summary['weekly'][week_label] = {
                'ë‹¹ë…„': int(curr),
                'ì „ë…„': int(prev),
                'YOY': yoy
            }
        
        for channel_name in df['ì±„ë„ëª…'].unique():
            ch_df = df[df['ì±„ë„ëª…'] == channel_name]
            curr = ch_df[ch_df['êµ¬ë¶„'] == 'ë‹¹ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
            prev = ch_df[ch_df['êµ¬ë¶„'] == 'ì „ë…„']['ì‹¤íŒë§¤ì¶œ'].sum()
            yoy = round((curr - prev) / prev * 100, 2) if prev != 0 else 0
            
            total_summary['channels'][channel_name] = {
                'ë‹¹ë…„': int(curr),
                'ì „ë…„': int(prev),
                'YOY': yoy
            }
        
        # JavaScript ê°ì²´ êµ¬ì¡°
        js_data = {
            'updateDate': update_date.strftime('%Y-%m-%d'),
            'period': {
                'start': str(week_end_dates[0].date() - timedelta(days=6)),
                'end': str(week_end_dates[-1].date())
            },
            'weeks': weeks,
            'channelMapping': channel_mapping,
            'brands': list(df['ë¸Œëœë“œ'].unique()),
            'channels': list(df['ì±„ë„ëª…'].unique()),
            'summary': {
                'total': total_summary,
                'byBrand': brand_summary
            },
            'rawData': data_list
        }
        
        # JavaScript íŒŒì¼ ë‚´ìš© ìƒì„±
        js_content = f"""// ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ ë°ì´í„°
// ìë™ ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
// ì—…ë°ì´íŠ¸ì¼ì: {update_date.strftime('%Y-%m-%d')}

const weeklySalesTrend = {json.dumps(js_data, ensure_ascii=False, indent=2)};

// Dashboard.htmlì—ì„œ ì‚¬ìš©
if (typeof window !== 'undefined') {{
    window.weeklySalesTrend = weeklySalesTrend;
}}

// Node.js í™˜ê²½ ì§€ì›
if (typeof module !== 'undefined' && module.exports) {{
    module.exports = weeklySalesTrend;
}}
"""
        
        # JavaScript íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print(f"âœ… {description} ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {output_path.stat().st_size / 1024:.2f} KB")
        
        # JSON íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (public/data/YYYYMMDD/weekly_trend.json)
        date_param = update_date.strftime('%Y%m%d')
        json_dir = Path(os.path.dirname(os.path.dirname(__file__))) / "public" / "data" / date_param
        json_dir.mkdir(parents=True, exist_ok=True)
        json_path = json_dir / "weekly_trend.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(js_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
        print(f"   íŒŒì¼ í¬ê¸°: {json_path.stat().st_size / 1024:.2f} KB")
        
    except Exception as e:
        print(f"âŒ JS ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ ë°ì´í„°ë¥¼ Snowflakeì—ì„œ ì¡°íšŒí•˜ì—¬ CSVë¡œ ë‹¤ìš´ë¡œë“œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/download_weekly_sales_trend.py 2025-11-24
  python scripts/download_weekly_sales_trend.py  # ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
  
ì„¤ëª…:
  ì—…ë°ì´íŠ¸ì¼ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ì£¼ì°¨ê¹Œì§€ì˜ 9ì£¼ì¹˜ ë§¤ì¶œ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
  - ë‹¹ë…„ ë§¤ì¶œê³¼ ì „ë…„ ë™ì£¼ì°¨ ë§¤ì¶œì„ ëª¨ë‘ ì¡°íšŒ
  - Xì¶•ì—ëŠ” ì£¼ì°¨ì¢…ë£Œ ì¼ìš”ì¼ ë‚ ì§œ í‘œì‹œ
  - YOY(ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥ ) ê³„ì‚°
        """
    )
    
    parser.add_argument(
        'update_date',
        type=str,
        nargs='?',
        default=None,
        help='ì—…ë°ì´íŠ¸ì¼ì (ì˜ˆ: 2025-11-24, ê¸°ë³¸ê°’: ì˜¤ëŠ˜)'
    )
    
    parser.add_argument(
        '--weeks',
        type=int,
        default=9,
        help='ë¶„ì„í•  ì£¼ì°¨ ìˆ˜ (ê¸°ë³¸ê°’: 9)'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: raw/YYYYMM/ETC)'
    )
    
    args = parser.parse_args()
    
    # ì—…ë°ì´íŠ¸ì¼ì íŒŒì‹±
    if args.update_date:
        try:
            update_date = datetime.strptime(args.update_date, '%Y-%m-%d')
        except ValueError:
            print(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.update_date} (YYYY-MM-DD í˜•ì‹ í•„ìš”)")
            sys.exit(1)
    else:
        update_date = datetime.now()
    
    # ì£¼ì°¨ ì¢…ë£Œì¼ ê³„ì‚°
    week_end_dates, start_date, end_date = calculate_week_end_dates(update_date, args.weeks)
    
    print("=" * 70)
    print("ğŸ“Š ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    print("=" * 70)
    print(f"ğŸ“… ì—…ë°ì´íŠ¸ì¼ì: {update_date.strftime('%Y-%m-%d')} ({['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'][update_date.weekday()]})")
    print(f"ğŸ“… ë¶„ì„ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    print(f"ğŸ“… ë¶„ì„ ì£¼ì°¨ ìˆ˜: {args.weeks}ì£¼")
    print(f"ğŸ“… Xì¶• í‘œì‹œ (ì£¼ì°¨ ì¢…ë£Œ ì¼ìš”ì¼):")
    for i, d in enumerate(week_end_dates, 1):
        print(f"   {i}. {d.strftime('%Y-%m-%d')} ({d.month}/{d.day})")
    print()
    
    conn = None
    try:
        # ì±„ë„ë§ˆìŠ¤í„° ë¡œë“œ
        channel_mapping = load_channel_master()
        
        # Snowflake ì—°ê²°
        conn = get_snowflake_connection()
        
        # ì›¨ì–´í•˜ìš°ìŠ¤ ìƒíƒœ í™•ì¸
        print("\nğŸ­ ì›¨ì–´í•˜ìš°ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
        cursor = conn.cursor()
        try:
            cursor.execute("SELECT CURRENT_WAREHOUSE(), CURRENT_DATABASE()")
            wh_info = cursor.fetchone()
            print(f"   ì›¨ì–´í•˜ìš°ìŠ¤: {wh_info[0]}")
            print(f"   ë°ì´í„°ë² ì´ìŠ¤: {wh_info[1]}")
            cursor.close()
        except Exception as e:
            print(f"   âš ï¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰
        query = get_weekly_sales_query(start_date, end_date)
        print("\nğŸ“ ìƒì„±ëœ ì¿¼ë¦¬:")
        print("-" * 50)
        print(query[:500] + "..." if len(query) > 500 else query)
        print("-" * 50)
        
        df = execute_query_to_dataframe(conn, query)
        
        if df.empty:
            print("âš ï¸ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(0)
        
        # ë°ì´í„° ì²˜ë¦¬ (ì±„ë„ ë§¤í•‘ í¬í•¨)
        result_df = process_weekly_sales_data(df, channel_mapping)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²°ì • (í‰ê°€ì›” ì‚¬ìš©)
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            # ì—…ë°ì´íŠ¸ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            date_str = update_date.strftime('%Y%m%d')
            # í‰ê°€ì›”(analysis_month) ì¶”ì¶œ (metadata.jsonì—ì„œ ì½ê±°ë‚˜ ê³„ì‚°)
            analysis_month = extract_year_month_from_date(date_str)
            output_dir = project_root / "raw" / analysis_month / "ETC"
        
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ë‚ ì§œ
        date_suffix = update_date.strftime('%Y%m%d')
        
        # CSV íŒŒì¼ ì €ì¥
        print("\n" + "=" * 70)
        print("ğŸ’¾ íŒŒì¼ ì €ì¥")
        print("=" * 70)
        
        # CSV íŒŒì¼ ì €ì¥
        save_to_csv(
            result_df,
            output_dir / f"weekly_sales_trend_{date_suffix}.csv",
            "ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ (CSV)"
        )
        
        # JS íŒŒì¼ ì €ì¥ (public í´ë”ì—)
        js_output_path = project_root / "public" / f"weekly_sales_trend_{date_suffix}.js"
        save_to_js(
            result_df,
            js_output_path,
            update_date,
            week_end_dates,
            channel_mapping,
            "ì£¼ì°¨ë³„ ë§¤ì¶œì¶”ì„¸ (JS)"
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 70)
        print("ğŸ“Š ë°ì´í„° ìš”ì•½")
        print("=" * 70)
        
        print(f"\nğŸ“‹ ì´ ë°ì´í„° ê±´ìˆ˜: {len(result_df):,}ê±´")
        
        print("\nğŸ·ï¸ ë¸Œëœë“œ ëª©ë¡:")
        brands = result_df['ë¸Œëœë“œ'].unique()
        for brand in sorted(brands):
            print(f"   - {brand}")
        
        print("\nğŸª ìœ í†µì±„ë„ â†’ ì±„ë„ëª… ë§¤í•‘:")
        channel_df = result_df[['ìœ í†µì±„ë„', 'ì±„ë„ëª…']].drop_duplicates()
        for _, row in channel_df.sort_values('ìœ í†µì±„ë„').iterrows():
            print(f"   - {row['ìœ í†µì±„ë„']} â†’ {row['ì±„ë„ëª…']}")
        
        # ì£¼ì°¨ë³„ ì „ì²´ ë§¤ì¶œ ê³„ì‚°
        print("\nğŸ“ˆ ì£¼ì°¨ë³„ ì „ì²´ ë§¤ì¶œ (ë°±ë§Œì›):")
        print(f"   ì£¼ì°¨: {' | '.join([f'{d.month}/{d.day}' for d in week_end_dates])}")
        
        # ë‹¹ë…„
        curr_df = result_df[result_df['êµ¬ë¶„'] == 'ë‹¹ë…„'].copy()
        # ì¢…ë£Œì¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        curr_df['ì¢…ë£Œì¼_str'] = curr_df['ì¢…ë£Œì¼'].astype(str)
        curr_by_week = curr_df.groupby('ì¢…ë£Œì¼_str')['ì‹¤íŒë§¤ì¶œ'].sum() / 1_000_000
        curr_values = [str(round(curr_by_week.get(d.strftime('%Y-%m-%d'), 0), 1)) for d in week_end_dates]
        print(f"   ë‹¹ë…„: {' | '.join(curr_values)}")
        
        # ì „ë…„
        prev_df = result_df[result_df['êµ¬ë¶„'] == 'ì „ë…„'].copy()
        # ì¢…ë£Œì¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        prev_df['ì¢…ë£Œì¼_str'] = prev_df['ì¢…ë£Œì¼'].astype(str)
        prev_by_week = prev_df.groupby('ì¢…ë£Œì¼_str')['ì‹¤íŒë§¤ì¶œ'].sum() / 1_000_000
        prev_values = [str(round(prev_by_week.get(d.strftime('%Y-%m-%d'), 0), 1)) for d in week_end_dates]
        print(f"   ì „ë…„: {' | '.join(prev_values)}")
        
        print("\n" + "=" * 70)
        print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print("=" * 70)
        
    except Exception as e:
        print()
        print("=" * 70)
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("=" * 70)
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ Snowflake ì—°ê²° ì¢…ë£Œ")


if __name__ == "__main__":
    main()

