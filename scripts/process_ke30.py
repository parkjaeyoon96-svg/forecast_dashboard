"""
SAP KE30 ìì¬ë³„ ì†ìµ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
"""
import pandas as pd
import numpy as np
import json
from datetime import datetime
import os

def load_ke30_data(file_path='raw/sap_ke30.csv'):
    """KE30 CSV íŒŒì¼ ë¡œë“œ"""
    try:
        # CSV ì¸ì½”ë”© ìë™ ê°ì§€ (SAPëŠ” ë³´í†µ cp949 ë˜ëŠ” utf-8)
        try:
            df = pd.read_csv(file_path, encoding='cp949')
        except:
            df = pd.read_csv(file_path, encoding='utf-8')
        
        print(f"âœ… KE30 ë°ì´í„° ë¡œë“œ: {len(df)}ê±´")
        print(f"   ì»¬ëŸ¼: {df.columns.tolist()}")
        return df
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def clean_ke30_data(df):
    """ë°ì´í„° ì •ì œ"""
    print("\nğŸ”§ ë°ì´í„° ì •ì œ ì¤‘...")
    
    # 1. ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    df.columns = df.columns.str.strip()
    
    # 2. ìˆ«ì ì»¬ëŸ¼ ì •ë¦¬ (ì‰¼í‘œ ì œê±°, ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜)
    numeric_columns = df.select_dtypes(include=['object']).columns
    for col in numeric_columns:
        # ìˆ«ìì²˜ëŸ¼ ë³´ì´ëŠ” ì»¬ëŸ¼ë§Œ ë³€í™˜
        try:
            # ì‰¼í‘œ, ê³µë°± ì œê±° í›„ ìˆ«ì ë³€í™˜ ì‹œë„
            df[col] = df[col].astype(str).str.replace(',', '').str.replace(' ', '')
            df[col] = pd.to_numeric(df[col], errors='ignore')
        except:
            pass
    
    # 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna(0)
    
    # 4. ì¤‘ë³µ ì œê±°
    before_count = len(df)
    df = df.drop_duplicates()
    after_count = len(df)
    if before_count != after_count:
        print(f"   ì¤‘ë³µ ì œê±°: {before_count - after_count}ê±´")
    
    print(f"âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ: {len(df)}ê±´")
    return df

def aggregate_ke30_data(df):
    """
    ìì¬ë³„ ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ê³  ëŒ€ì‹œë³´ë“œìš© KPI ê³„ì‚°
    """
    print("\nğŸ“Š ë°ì´í„° ì§‘ê³„ ì¤‘...")
    
    # ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
    # í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„° ê¸°ì¤€
    
    # 1. ìì¬ë³„ ì§‘ê³„
    agg_config = {
        'ìˆ˜ëŸ‰': 'sum',
        'ë§¤ì¶œì•¡': 'sum',
        'ë§¤ì¶œì›ê°€': 'sum',
        'íŒë§¤ê´€ë¦¬ë¹„': 'sum',
        'ì˜ì—…ì´ìµ': 'sum',
        'ê³ ì •ë¹„': 'sum',
        'ë³€ë™ë¹„': 'sum',
    }
    
    material_summary = df.groupby(['ìì¬ì½”ë“œ', 'ìì¬ëª…']).agg(agg_config).reset_index()
    
    # ê³„ì‚° ì»¬ëŸ¼ ì¶”ê°€
    material_summary['ë§¤ì¶œì´ì´ìµ'] = material_summary['ë§¤ì¶œì•¡'] - material_summary['ë§¤ì¶œì›ê°€']
    material_summary['ì´ìµë¥ '] = (material_summary['ì˜ì—…ì´ìµ'] / material_summary['ë§¤ì¶œì•¡'] * 100).round(2)
    material_summary['ì›ê°€ìœ¨'] = (material_summary['ë§¤ì¶œì›ê°€'] / material_summary['ë§¤ì¶œì•¡'] * 100).round(2)
    
    print(f"   - ìì¬ë³„ ì§‘ê³„: {len(material_summary)}ê±´")
    
    # 2. ì¼ìë³„ ì§‘ê³„ (íŠ¸ë Œë“œ ë¶„ì„ìš©)
    if 'ì¼ì' in df.columns:
        daily_summary = df.groupby('ì¼ì').agg({
            'ë§¤ì¶œì•¡': 'sum',
            'ë§¤ì¶œì›ê°€': 'sum',
            'ì˜ì—…ì´ìµ': 'sum',
        }).reset_index()
        daily_summary = daily_summary.sort_values('ì¼ì')
        print(f"   - ì¼ìë³„ ì§‘ê³„: {len(daily_summary)}ê±´")
    else:
        daily_summary = None
    
    print(f"âœ… ë°ì´í„° ì§‘ê³„ ì™„ë£Œ")
    
    return {
        'material_summary': material_summary,
        'daily_summary': daily_summary,
        'raw_data': df
    }

def enrich_with_master(df, master_data):
    """ë§ˆìŠ¤í„° ë°ì´í„°ì™€ ì¡°ì¸í•˜ì—¬ ì •ë³´ ë³´ê°•"""
    print("\nğŸ”— ë§ˆìŠ¤í„° ë°ì´í„° ì¡°ì¸ ì¤‘...")
    
    if master_data is None:
        print("   âš ï¸ ë§ˆìŠ¤í„° ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        return df
    
    # ì˜ˆì‹œ: ì œí’ˆ ë§ˆìŠ¤í„°ì™€ ì¡°ì¸
    # df = df.merge(master_data, left_on='ìì¬ì½”ë“œ', right_on='ì œí’ˆì½”ë“œ', how='left')
    
    return df

def validate_data(df):
    """ë°ì´í„° ê²€ì¦"""
    print("\nâœ“ ë°ì´í„° ê²€ì¦ ì¤‘...")
    
    issues = []
    
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
    # required_columns = ['ìì¬ì½”ë“œ', 'ë§¤ì¶œì•¡']
    # missing_columns = [col for col in required_columns if col not in df.columns]
    # if missing_columns:
    #     issues.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
    
    # 2. ìŒìˆ˜ ì²´í¬ (í•„ìš”í•œ ê²½ìš°)
    # if 'ë§¤ì¶œì•¡' in df.columns:
    #     negative_count = (df['ë§¤ì¶œì•¡'] < 0).sum()
    #     if negative_count > 0:
    #         issues.append(f"ìŒìˆ˜ ë§¤ì¶œì•¡: {negative_count}ê±´")
    
    # 3. ê²°ì¸¡ì¹˜ ì²´í¬
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        issues.append(f"ê²°ì¸¡ì¹˜ ë°œê²¬: {null_counts[null_counts > 0].to_dict()}")
    
    if issues:
        print("   âš ï¸ ê²€ì¦ ì´ìŠˆ:")
        for issue in issues:
            print(f"      - {issue}")
    else:
        print("   âœ… ê²€ì¦ í†µê³¼")
    
    return issues

def save_processed_data(df, output_path='data/ke30_processed.json'):
    """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘: {output_path}")
    
    # JSONìœ¼ë¡œ ë³€í™˜
    data_dict = {
        'data': df.to_dict('records'),
        'metadata': {
            'record_count': len(df),
            'columns': df.columns.tolist(),
            'processed_at': datetime.now().isoformat(),
            'data_types': df.dtypes.astype(str).to_dict()
        },
        'summary': {
            # ìš”ì•½ í†µê³„ (ì˜ˆì‹œ)
            # 'total_sales': float(df['ë§¤ì¶œì•¡'].sum()) if 'ë§¤ì¶œì•¡' in df.columns else 0,
            # 'total_cost': float(df['ë§¤ì¶œì›ê°€'].sum()) if 'ë§¤ì¶œì›ê°€' in df.columns else 0,
        }
    }
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data_dict, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {len(df)}ê±´")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("SAP KE30 ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_ke30_data('raw/sap_ke30.csv')
    if df is None:
        print("\nâŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("\nì‚¬ìš©ë²•:")
        print("  1. SAPì—ì„œ KE30 ë°ì´í„°ë¥¼ CSVë¡œ ë‹¤ìš´ë¡œë“œ")
        print("  2. raw/sap_ke30.csv íŒŒì¼ë¡œ ì €ì¥")
        print("  3. ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python scripts/process_ke30.py")
        return
    
    print("\nğŸ“‹ ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
    print(f"\në°ì´í„° íƒ€ì…:\n{df.dtypes}")
    
    # 2. ë°ì´í„° ì •ì œ
    df = clean_ke30_data(df)
    
    # 3. ë°ì´í„° ì§‘ê³„
    df = aggregate_ke30_data(df)
    
    # 4. ë§ˆìŠ¤í„° ë°ì´í„° ì¡°ì¸ (ì„ íƒì )
    # master_data = load_master_data()  # ë³„ë„ í•¨ìˆ˜ í•„ìš”
    # df = enrich_with_master(df, master_data)
    
    # 5. ë°ì´í„° ê²€ì¦
    issues = validate_data(df)
    
    # 6. ê²°ê³¼ ì €ì¥
    save_processed_data(df)
    
    print("\n" + "=" * 60)
    print("âœ… KE30 ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nì²˜ë¦¬ ê²°ê³¼:")
    print(f"  - ìµœì¢… ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    print(f"  - ì¶œë ¥ íŒŒì¼: data/ke30_processed.json")
    print(f"  - ê²€ì¦ ì´ìŠˆ: {len(issues)}ê±´")
    
    if issues:
        print("\nâš ï¸ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!")

if __name__ == "__main__":
    main()

